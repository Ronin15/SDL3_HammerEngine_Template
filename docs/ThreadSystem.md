# ThreadSystem Documentation

## Overview

The Hammer Engine ThreadSystem is a robust, production-ready thread pool implementation designed for high-performance game development. It provides efficient task-based concurrency with priority-based scheduling, WorkerBudget resource allocation, and comprehensive performance monitoring. The design emphasizes reliability, maintainability, and consistent performance across diverse hardware configurations.

## Architecture Overview

### Core Components Hierarchy

```
ThreadSystem (Singleton)
‚îú‚îÄ‚îÄ ThreadPool (Worker thread management)
‚îÇ   ‚îú‚îÄ‚îÄ TaskQueue (Global priority-based queue)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Priority Queues [0-4] (Critical ‚Üí Idle)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Per-Priority Mutexes (Reduced contention)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Statistics Tracking
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ Profiling System
‚îÇ   ‚îî‚îÄ‚îÄ Worker Threads [N]
‚îÇ       ‚îú‚îÄ‚îÄ Priority-Based Task Processing
‚îÇ       ‚îú‚îÄ‚îÄ Exponential Backoff
‚îÇ       ‚îî‚îÄ‚îÄ Exception Handling
‚îî‚îÄ‚îÄ WorkerBudget System (Intelligent resource allocation)
    ‚îú‚îÄ‚îÄ AI: 45% allocation
    ‚îú‚îÄ‚îÄ Particles: 25% allocation
    ‚îú‚îÄ‚îÄ Events: 20% allocation
    ‚îú‚îÄ‚îÄ Engine: 1-2 workers reserved
    ‚îî‚îÄ‚îÄ Buffer: Dynamic burst capacity
```

### Class Responsibilities

| Class | Responsibility | Key Features |
|-------|---------------|--------------|
| **ThreadSystem** | Singleton API manager | Initialization, cleanup, public interface |
| **ThreadPool** | Worker thread lifecycle | Thread creation, task distribution, graceful shutdown |
| **TaskQueue** | Priority-based queuing | 5 priority levels, per-priority mutexes, capacity management |
| **WorkerBudget** | Resource allocation | Hardware-adaptive allocation with dynamic buffer usage |
| **PrioritizedTask** | Task wrapper | Priority, timing, description, FIFO within priority |

### Performance Characteristics

| Metric | Value | Notes |
|--------|-------|-------|
| **Memory Overhead** | <0.5KB | Minimal static system overhead |
| **CPU Overhead** | <0.05% | Per task processing cost |
| **Throughput** | 15,000-25,000 tasks/sec | Small tasks (100-1000 ops) |
| **Load Balance Efficiency** | 85%+ | Priority-based distribution |
| **Scalability** | 75-95% efficiency | 2-16+ cores, reliable scaling |

## Implementation Details

### Memory Management Strategy

**Queue Capacity Management:**
```cpp
// Dynamic growth strategy
if (queue.size() >= (m_desiredCapacity * 90) / 100) { // 90% threshold
    size_t newCapacity = queue.capacity() * 2;
    queue.reserve(newCapacity);
}
```

**Memory Efficiency Features:**
- **Pre-allocation**: `reserveQueueCapacity()` for known workloads
- **Exponential Growth**: Doubles capacity when 90% full
- **Move Semantics**: Extensive use of `std::move` to avoid copies
- **Minimal Overhead**: <1KB total system overhead

### Thread Safety Implementation

**Synchronization Primitives:**
- **Per-Priority Mutexes**: Reduces contention between priority levels
- **Atomic Counters**: Lock-free statistics tracking
- **Condition Variables**: Efficient thread wake-up
- **Memory Ordering**: Explicit memory ordering for performance

**Lock-Free Operations:**
```cpp
// Atomic task counting
m_totalTasksProcessed.fetch_add(1, std::memory_order_relaxed);

// Lock-free stopping check
bool isStopping() const {
    return stopping.load(std::memory_order_acquire);
}
```

## Key Features

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    ThreadSystem (Singleton)                 ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ  ‚îÇ   ThreadPool    ‚îÇ    ‚îÇ        TaskQueue                ‚îÇ ‚îÇ
‚îÇ  ‚îÇ                 ‚îÇ    ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê‚îÇ ‚îÇ
‚îÇ  ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ    ‚îÇ  ‚îÇ Priority Queues (0-4)       ‚îÇ‚îÇ ‚îÇ
‚îÇ  ‚îÇ ‚îÇ Worker 0    ‚îÇ ‚îÇ    ‚îÇ  ‚îÇ Critical ‚îÇ High ‚îÇ Normal    ‚îÇ‚îÇ ‚îÇ
‚îÇ  ‚îÇ ‚îÇ Worker 1    ‚îÇ ‚îÇ‚óÑ‚îÄ‚îÄ‚îÄ‚î§  ‚îÇ Low      ‚îÇ Idle ‚îÇ           ‚îÇ‚îÇ ‚îÇ
‚îÇ  ‚îÇ ‚îÇ Worker N    ‚îÇ ‚îÇ    ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò‚îÇ ‚îÇ
‚îÇ  ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                                        ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê‚îÇ
‚îÇ  ‚îÇ            WorkerBudget Allocation                      ‚îÇ‚îÇ
‚îÇ  ‚îÇ   Engine: 10% ‚îÇ AI: 60% ‚îÇ Events: 30% ‚îÇ Buffer: Auto    ‚îÇ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
```

## Key Features

- **üîÑ Automatic Thread Pool Management**: Optimal sizing based on CPU cores with intelligent worker allocation
- **‚ö° Priority-Based Scheduling**: 5-level priority system with separate queues for minimal contention
- **üîÄ Optimized Task Distribution**: Efficient batch processing with WorkerBudget-based load balancing
- **üìä Performance Monitoring**: Built-in profiling, statistics tracking, and performance analytics
- **üõ°Ô∏è Thread Safety**: Lock-free operations where possible with comprehensive synchronization
- **üéØ Engine Integration**: Seamless integration with AIManager, EventManager, and core systems
- **‚öôÔ∏è WorkerBudget System**: Intelligent resource allocation across engine subsystems (60% AI, 30% Events, 10% Engine coordination)
- **üîß Clean Shutdown**: Graceful termination with proper resource cleanup

## Quick Start

### Basic Initialization

```cpp
#include "core/ThreadSystem.hpp"
using namespace HammerEngine;

// Initialize with default settings (recommended)
if (!ThreadSystem::Instance().init()) {
    THREADSYSTEM_ERROR("Failed to initialize ThreadSystem!");
    return false;
}

// Initialize with custom parameters
bool success = ThreadSystem::Instance().init(
    4096,                    // Queue capacity
    8,                       // Custom thread count (0 = auto-detect)
    true                     // Enable profiling
);

// Verify initialization
if (success) {
    unsigned int threads = ThreadSystem::Instance().getThreadCount();
    THREADSYSTEM_INFO("ThreadSystem initialized with " + std::to_string(threads) + " threads");
}
```

### Basic Task Submission

```cpp
// Simple fire-and-forget task
ThreadSystem::Instance().enqueueTask([]() {
    // Your task code here
    processGameLogic();
}, TaskPriority::Normal, "Game Logic Update");

// Task with return value
auto future = ThreadSystem::Instance().enqueueTaskWithResult([](int value) -> int {
    return calculateComplexValue(value);
}, TaskPriority::High, "Complex Calculation", 42);

// Retrieve result (blocks until complete)
int result = future.get();
```

### Batch Processing Example

```cpp
// Process large collections efficiently
std::vector<Entity*> entities = getActiveEntities();
size_t batchSize = entities.size() / ThreadSystem::Instance().getThreadCount();

for (size_t i = 0; i < entities.size(); i += batchSize) {
    ThreadSystem::Instance().enqueueTask([=]() {
        size_t end = std::min(i + batchSize, entities.size());
        for (size_t j = i; j < end; ++j) {
            entities[j]->update();
        }
    }, TaskPriority::Normal, "Entity Batch " + std::to_string(i / batchSize));
}
```

## Task Priority System

### Priority Levels

```cpp
enum class TaskPriority : int {
    Critical = 0,   // Must execute ASAP (rendering, input handling)
    High = 1,       // Important tasks (physics, animation)
    Normal = 2,     // Default priority for most tasks
    Low = 3,        // Background tasks (asset loading)
    Idle = 4        // Only execute when nothing else is pending
};
```

### Priority Guidelines

| Priority | Use Case | Examples | Queue Behavior |
|----------|----------|----------|----------------|
| **Critical** | Mission-critical operations | Input handling, rendering pipeline | Immediate processing, notify all threads |
| **High** | Important game operations | Physics updates, combat AI | High priority processing, notify all threads |
| **Normal** | Standard game logic | Entity updates, standard AI | Default processing, single notification |
| **Low** | Background operations | Asset loading, cleanup | Lower priority, single notification |
| **Idle** | Maintenance tasks | Memory cleanup, cache optimization | Lowest priority, execute when idle |

### Priority Usage Examples

```cpp
// Critical: Frame-critical operations
ThreadSystem::Instance().enqueueTask([]() {
    renderer.updateRenderQueue();
}, TaskPriority::Critical, "Render Queue Update");

// High: Player-visible operations
ThreadSystem::Instance().enqueueTask([]() {
    player.updateCombatSystem();
}, TaskPriority::High, "Player Combat");

// Normal: Background game logic
ThreadSystem::Instance().enqueueTask([]() {
    updateNPCAI(npcList);
}, TaskPriority::Normal, "NPC AI Update");

// Low: Resource management
ThreadSystem::Instance().enqueueTask([]() {
    assetManager.loadBackgroundAssets();
}, TaskPriority::Low, "Background Asset Loading");

// Idle: Cleanup operations
ThreadSystem::Instance().enqueueTask([]() {
    memoryManager.defragmentMemory();
}, TaskPriority::Idle, "Memory Defragmentation");
```



## WorkerBudget System

### Overview

The WorkerBudget system provides intelligent resource allocation across engine subsystems, ensuring optimal performance distribution while maintaining system responsiveness.

### Allocation Strategy

The system uses a tiered allocation strategy that adapts to hardware capabilities:

**Tier 1 (‚â§1 workers)**: Ultra low-end systems
- GameEngine: 1 worker (all available)
- AI: 0 workers (single-threaded fallback)
- Events: 0 workers (single-threaded fallback)

**Tier 2 (2-4 workers)**: Low-end systems
- GameEngine: 1 worker (minimum required)
- AI: 1 worker if available
- Events: 0 workers (shares with AI or single-threaded)

**Tier 3 (5+ workers)**: High-end systems
- GameEngine: 2 workers (optimal for coordination)
- AI: 60% of remaining workers after engine reservation
- Events: 30% of remaining workers after engine reservation
- Buffer: Remaining workers for burst capacity

```cpp
struct WorkerBudget {
    size_t totalWorkers;      // Total available worker threads
    size_t engineReserved;    // Reserved for critical engine operations
    size_t aiAllocated;       // Allocated for AI subsystem (60% of remaining)
    size_t eventAllocated;    // Allocated for event processing (30% of remaining)
    size_t remaining;         // Buffer workers for burst capacity

    size_t getOptimalWorkerCount(size_t baseAllocation, size_t workloadSize, size_t workloadThreshold) const;
    bool hasBufferCapacity() const { return remaining > 0; }
    size_t getMaxWorkerCount(size_t baseAllocation) const { return baseAllocation + remaining; }
};
```

### Hardware Tier Classification

| Hardware Tier | CPU Cores/Threads | Worker Allocation | AI Workers | Event Workers | Engine Reserved | Buffer |
|---------------|-------------------|-------------------|------------|---------------|-----------------|--------|
| **Ultra Low-End** | 1-2 cores/2-4 threads | 1-3 workers | 0-1 | 0-1 | 1 | 0 |
| **Low-End** | 2-4 cores/4-8 threads | 3-7 workers | 1-3 | 1 | 1-2 | 0-1 |
| **Mid-Range** | 4-6 cores/8-12 threads | 7-11 workers | 3-6 | 1-3 | 2 | 1-2 |
| **High-End** | 6-8 cores/12-16 threads | 11-15 workers | 5-7 | 2-4 | 2 | 2-3 |
| **Enthusiast** | 8+ cores/16+ threads | 15+ workers | 8+ | 4+ | 2 | 3+ |

### Real-World Allocation Examples

```cpp
// 4-core/8-thread system (7 workers available)
WorkerBudget budget = {
    .totalWorkers = 7,
    .engineReserved = 2,     // 29% - Enhanced engine capacity for mid-tier systems
    .aiAllocated = 3,        // 43% - AI processing (60% of remaining 5 workers)
    .eventAllocated = 1,     // 14% - Event handling (30% of remaining 5 workers)
    .remaining = 1           // 14% - Buffer for burst workloads
};

// 8-core/16-thread system (15 workers available)
WorkerBudget budget = {
    .totalWorkers = 15,
    .engineReserved = 2,     // 13% - Enhanced engine capacity
    .aiAllocated = 7,        // 47% - AI processing (60% of remaining 13 workers)
    .eventAllocated = 3,     // 20% - Event handling (30% of remaining 13 workers)
    .remaining = 3           // 20% - Buffer for burst workloads
};

// 2-core/4-thread system (3 workers available) - Low-end
WorkerBudget budget = {
    .totalWorkers = 3,
    .engineReserved = 1,     // 33% - Critical engine operations
    .aiAllocated = 1,        // 33% - Minimal AI processing
    .eventAllocated = 1,     // 33% - Minimal event handling
    .remaining = 0           // No buffer available
};

// 6-core/12-thread system (11 workers available) - High-end
WorkerBudget budget = {
    .totalWorkers = 11,
    .engineReserved = 2,     // 18% - Enhanced engine capacity
    .aiAllocated = 5,        // 45% - AI processing (60% of remaining 9 workers)
    .eventAllocated = 2,     // 18% - Event handling (30% of remaining 9 workers)
    .remaining = 2           // 18% - Buffer for burst workloads
};
```

### Buffer Thread Utilization

**Current Performance**: Achieves 4-6% CPU usage with optimized WorkerBudget allocation and batch processing.

```cpp
void AIManager::update() {
    size_t entityCount = getActiveEntityCount();
    if (entityCount == 0) return;

    // Calculate optimal worker allocation using WorkerBudget system
    auto& threadSystem = ThreadSystem::Instance();
    size_t availableWorkers = static_cast<size_t>(threadSystem.getThreadCount());
    WorkerBudget budget = calculateWorkerBudget(availableWorkers);

    // Use WorkerBudget's intelligent buffer allocation
    size_t optimalWorkerCount = budget.getOptimalWorkerCount(budget.aiAllocated, entityCount, 1000);

    // Optimal batching: 2-4 large batches for best performance
    size_t minEntitiesPerBatch = 1000;
    size_t batchCount = std::min(optimalWorkerCount, entityCount / minEntitiesPerBatch);
    batchCount = std::max(size_t(1), std::min(batchCount, size_t(4))); // Cap at 4 batches

    size_t entitiesPerBatch = entityCount / batchCount;
    size_t remainingEntities = entityCount % batchCount;

    // Submit optimized batches
    for (size_t i = 0; i < batchCount; ++i) {
        size_t start = i * entitiesPerBatch;
        size_t end = start + entitiesPerBatch;

        // Add remaining entities to last batch
        if (i == batchCount - 1) {
            end += remainingEntities;
        }

        threadSystem.enqueueTask([this, start, end, deltaTime]() {
            processBatch(start, end, deltaTime);
        }, TaskPriority::High, "AI_OptimalBatch");
    }
}
```

**Key Optimizations:**
- **Threshold-based buffer allocation**: Uses 1000 entity threshold for buffer worker activation
- **Optimal batch sizing**: 2-4 large batches (1000+ entities each) for maximum efficiency
- **Lock-free processing**: Pre-cached entity data eliminates lock contention
- **Distance calculation optimization**: Only every 4th frame, active entities only
- **Pure distance culling**: Removed unnecessary frame counting for better performance

## API Reference

### Core Initialization Methods

```cpp
class ThreadSystem {
public:
    // Singleton access
    static ThreadSystem& Instance();
    static bool Exists();

    // Initialization and cleanup
    bool init(size_t queueCapacity = DEFAULT_QUEUE_CAPACITY,
              unsigned int customThreadCount = 0,
              bool enableProfiling = false);
    void clean();

    // System status
    bool isShutdown() const;
    unsigned int getThreadCount() const;
};
```

### Task Management Methods

```cpp
// Basic task submission
void enqueueTask(std::function<void()> task,
                 TaskPriority priority = TaskPriority::Normal,
                 const std::string& description = "");

// Task with result
template<class F, class... Args>
auto enqueueTaskWithResult(F&& f,
                          TaskPriority priority = TaskPriority::Normal,
                          const std::string& description = "",
                          Args&&... args)
    -> std::future<typename std::invoke_result<F, Args...>::type>;
```

### Queue Management Methods

```cpp
// Queue status
bool isBusy() const;
size_t getQueueSize() const;
size_t getQueueCapacity() const;
bool reserveQueueCapacity(size_t capacity);

// Statistics
size_t getTotalTasksProcessed() const;
size_t getTotalTasksEnqueued() const;

// Debug and profiling
void setDebugLogging(bool enable);
bool isDebugLoggingEnabled() const;
```

## Engine Integration

### AIManager Integration

```cpp
class AIManager {
private:
    void updateEntitiesWithOptimalBatching(float deltaTime) {
        size_t entityCount = m_storage.size();
        if (entityCount == 0) return;

        // Use WorkerBudget system for optimal resource allocation
        auto& threadSystem = ThreadSystem::Instance();
        size_t availableWorkers = static_cast<size_t>(threadSystem.getThreadCount());
        WorkerBudget budget = calculateWorkerBudget(availableWorkers);

        // Get optimal worker count with buffer allocation
        size_t optimalWorkerCount = budget.getOptimalWorkerCount(budget.aiAllocated, entityCount, 1000);

        // Optimal batching: 2-4 large batches for maximum efficiency
        size_t minEntitiesPerBatch = 1000;
        size_t batchCount = std::min(optimalWorkerCount, entityCount / minEntitiesPerBatch);
        batchCount = std::max(size_t(1), std::min(batchCount, size_t(4)));

        size_t entitiesPerBatch = entityCount / batchCount;
        size_t remainingEntities = entityCount % batchCount;

        // Submit optimized batches with High priority
        for (size_t i = 0; i < batchCount; ++i) {
            size_t start = i * entitiesPerBatch;
            size_t end = start + entitiesPerBatch;

            if (i == batchCount - 1) {
                end += remainingEntities;
            }

            threadSystem.enqueueTask([this, start, end, deltaTime]() {
                processBatch(start, end, deltaTime);
            }, TaskPriority::High, "AI_OptimalBatch");
        }
    }

    // Optimized batch processing with lock-free entity caching
    void processBatch(size_t start, size_t end, float deltaTime) {
        // Pre-cache entities and behaviors to reduce lock contention
        std::vector<EntityPtr> batchEntities;
        std::vector<std::shared_ptr<AIBehavior>> batchBehaviors;

        // Single lock acquisition for entire batch
        {
            std::shared_lock<std::shared_mutex> lock(m_entitiesMutex);
            for (size_t i = start; i < end && i < m_storage.size(); ++i) {
                batchEntities.push_back(m_storage.entities[i]);
                batchBehaviors.push_back(m_storage.behaviors[i]);
            }
        }

        // Process entities without locks using pre-calculated distance thresholds
        float maxDistSquared = m_maxUpdateDistance.load() * m_maxUpdateDistance.load();

        for (size_t idx = 0; idx < batchEntities.size(); ++idx) {
            EntityPtr entity = batchEntities[idx];
            auto behavior = batchBehaviors[idx];

            if (entity && behavior) {
                // Pure distance-based culling (no frame counting)
                bool shouldUpdate = true;
                if (hasPlayer) {
                    float distanceSquared = calculateDistanceSquared(entity->getPosition());
                    shouldUpdate = (distanceSquared <= maxDistSquared);
                }

                if (shouldUpdate) {
                    behavior->executeLogic(entity);
                    entity->update(deltaTime);
                }
            }
        }
    }
};
```

### EventManager Integration

```cpp
class EventManager {
private:
    void processEventsParallel() {
        auto eventBatches = partitionEventsByType(pendingEvents);

        for (const auto& [eventType, events] : eventBatches) {
            ThreadSystem::Instance().enqueueTask([=]() {
                for (const auto& event : events) {
                    processEvent(event);
                }
            }, getEventPriority(eventType),
               "Event_Batch_" + std::to_string(static_cast<int>(eventType)));
        }
    }

    TaskPriority getEventPriority(EventType type) {
        switch (type) {
            case EventType::Input: return TaskPriority::Critical;
            case EventType::Physics: return TaskPriority::High;
            case EventType::Audio: return TaskPriority::Normal;
            case EventType::UI: return TaskPriority::Low;
            default: return TaskPriority::Normal;
        }
    }
};
```

## Performance Optimization

### Best Practices

#### ‚úÖ Optimal Usage Patterns

```cpp
// 1. Batch similar operations
std::vector<Entity*> entities = getEntities();
size_t batchSize = entities.size() / ThreadSystem::Instance().getThreadCount();

for (size_t i = 0; i < entities.size(); i += batchSize) {
    ThreadSystem::Instance().enqueueTask([=]() {
        processBatch(entities, i, std::min(batchSize, entities.size() - i));
    }, TaskPriority::Normal, "Entity Processing Batch");
}

// 2. Use appropriate priorities
ThreadSystem::Instance().enqueueTask(criticalRenderTask,
                                   TaskPriority::Critical, "Render Update");
ThreadSystem::Instance().enqueueTask(backgroundLoadTask,
                                   TaskPriority::Low, "Asset Loading");

// 3. Process large workloads with optimal batching
for (int i = 0; i < 10000; ++i) {
    ThreadSystem::Instance().enqueueTask([=]() {
        processEntity(i);
    }, TaskPriority::Normal, "Entity_" + std::to_string(i));
}
// WorkerBudget system provides optimal resource allocation
```

#### ‚ö†Ô∏è Anti-Patterns to Avoid

```cpp
// ‚ùå Don't create too many tiny tasks
for (int i = 0; i < 100000; ++i) {
    ThreadSystem::Instance().enqueueTask([=]() {
        simpleOperation(i);  // Overhead > benefit
    });
}

// ‚ùå Don't block worker threads
ThreadSystem::Instance().enqueueTask([]() {
    std::this_thread::sleep_for(std::chrono::seconds(1)); // Wastes worker
});

// ‚ùå Don't use high priority for non-critical tasks
ThreadSystem::Instance().enqueueTask(backgroundTask,
                                   TaskPriority::Critical); // Wrong priority
```

### Performance Guidelines

| Workload Size | Recommendation | Expected Efficiency |
|---------------|----------------|-------------------|
| **1-100 tasks** | Single batch or sequential | 70-85% |
| **100-1,000 tasks** | Worker-count batches | 85-90% |
| **1,000+ tasks** | Individual task submission | 90%+ with optimal batching |
| **10,000+ tasks** | WorkerBudget allocation scenario | 95%+ efficiency |

### Memory Optimization

```cpp
// Reserve capacity for known workloads
ThreadSystem::Instance().reserveQueueCapacity(expectedTaskCount);

// Use move semantics for large captures
auto largeData = generateLargeDataSet();
ThreadSystem::Instance().enqueueTask([data = std::move(largeData)]() {
    processLargeData(data);
}, TaskPriority::Normal, "Large Data Processing");

// Avoid excessive task descriptions in release builds
#ifdef DEBUG
    std::string description = "Detailed debug info: " + generateDescription();
#else
    std::string description = "";
#endif
ThreadSystem::Instance().enqueueTask(task, priority, description);
```

## Thread Safety

### Safe Patterns

```cpp
// ‚úÖ Thread-safe singleton access
auto& threadSystem = ThreadSystem::Instance();

// ‚úÖ Atomic operations for shared state
std::atomic<int> sharedCounter{0};
ThreadSystem::Instance().enqueueTask([&]() {
    sharedCounter.fetch_add(1, std::memory_order_relaxed);
});

// ‚úÖ Mutex-protected critical sections
std::mutex dataMutex;
std::vector<int> sharedData;

ThreadSystem::Instance().enqueueTask([&]() {
    std::lock_guard<std::mutex> lock(dataMutex);
    sharedData.push_back(42);
});

// ‚úÖ Thread-local storage for worker-specific data
thread_local int workerSpecificData = 0;
ThreadSystem::Instance().enqueueTask([]() {
    workerSpecificData++; // Safe: each worker has its own copy
});
```

### Thread-Safe Operations

| Operation | Thread Safety | Notes |
|-----------|---------------|-------|
| `enqueueTask()` | ‚úÖ Fully thread-safe | Can be called from any thread |
| `enqueueTaskWithResult()` | ‚úÖ Fully thread-safe | Returns thread-safe future |
| `getQueueSize()` | ‚úÖ Thread-safe read | May be slightly outdated |
| `isBusy()` | ‚úÖ Thread-safe read | Atomic operation |
| `clean()` | ‚ö†Ô∏è Single thread only | Call only during shutdown |

## Error Handling

### Exception Safety

```cpp
// ThreadSystem provides strong exception safety
ThreadSystem::Instance().enqueueTask([]() {
    try {
        riskyOperation();
    } catch (const std::exception& e) {
        THREADSYSTEM_ERROR("Task failed: " + std::string(e.what()));
        handleError(e);
    }
}, TaskPriority::Normal, "Risky Operation");

// Worker threads are protected from task exceptions
ThreadSystem::Instance().enqueueTask([]() {
    throw std::runtime_error("Task exception");
    // Worker thread continues running normally
});
```

### Error Recovery Patterns

```cpp
// Retry pattern for critical operations
void executeWithRetry(std::function<void()> task, int maxRetries = 3) {
    ThreadSystem::Instance().enqueueTask([=]() {
        for (int attempt = 0; attempt < maxRetries; ++attempt) {
            try {
                task();
                return; // Success
            } catch (const std::exception& e) {
                if (attempt == maxRetries - 1) {
                    THREADSYSTEM_ERROR("Task failed after " + std::to_string(maxRetries) + " attempts");
                    throw;
                }
                THREADSYSTEM_WARN("Task attempt " + std::to_string(attempt + 1) + " failed, retrying...");
            }
        }
    }, TaskPriority::High, "Retry Task");
}
```

## Integration Examples

### Complete Game Loop Integration

```cpp
class GameEngine {
private:
    bool init() {
        // Initialize ThreadSystem early in engine startup
        if (!ThreadSystem::Instance().init(4096, 0, true)) {
            THREADSYSTEM_ERROR("Failed to initialize ThreadSystem");
            return false;
        }

        // Initialize other systems
        if (!initRenderer() || !initAIManager() || !initEventManager()) {
            return false;
        }

        THREADSYSTEM_INFO("Game engine initialized successfully");
        return true;
    }

    void update(float deltaTime) {
        // Update systems using ThreadSystem
        updateInput();      // Main thread

        // Parallel updates
        ThreadSystem::Instance().enqueueTask([=]() {
            aiManager.update(deltaTime);
        }, TaskPriority::High, "AI Update");

        ThreadSystem::Instance().enqueueTask([=]() {
            physicsManager.update(deltaTime);
        }, TaskPriority::High, "Physics Update");

        ThreadSystem::Instance().enqueueTask([=]() {
            audioManager.update(deltaTime);
        }, TaskPriority::Normal, "Audio Update");

        // Wait for critical updates before rendering
        while (ThreadSystem::Instance().isBusy()) {
            std::this_thread::yield();
        }

        render(); // Main thread
    }

    void cleanup() {
        // Clean shutdown - process remaining tasks
        while (ThreadSystem::Instance().isBusy()) {
            std::this_thread::sleep_for(std::chrono::milliseconds(10));
        }

        ThreadSystem::Instance().clean();
        THREADSYSTEM_INFO("Game engine shutdown complete");
    }
};
```

## Monitoring and Debugging

### Performance Monitoring

```cpp
void monitorThreadSystemPerformance() {
    auto& ts = ThreadSystem::Instance();

    // Basic statistics
    size_t queueSize = ts.getQueueSize();
    size_t processed = ts.getTotalTasksProcessed();
    size_t enqueued = ts.getTotalTasksEnqueued();

    // Performance metrics
    double throughput = static_cast<double>(processed) / getUptimeSeconds();
    double queueUtilization = static_cast<double>(queueSize) / ts.getQueueCapacity();

    THREADSYSTEM_INFO("Performance - Throughput: " + std::to_string(throughput) +
                     " tasks/sec, Queue: " + std::to_string(queueUtilization * 100) + "%");

    // Alert on performance issues
    if (queueUtilization > 0.8) {
        THREADSYSTEM_WARN("High queue utilization detected: " +
                         std::to_string(queueUtilization * 100) + "%");
    }
}
```

### Debug Information

```cpp
void debugThreadSystem() {
    auto& ts = ThreadSystem::Instance();

    // Enable detailed logging
    ts.setDebugLogging(true);

    // System status
    THREADSYSTEM_DEBUG("ThreadSystem Status:");
    THREADSYSTEM_DEBUG("  Workers: " + std::to_string(ts.getThreadCount()));
    THREADSYSTEM_DEBUG("  Queue Size: " + std::to_string(ts.getQueueSize()));
    THREADSYSTEM_DEBUG("  Queue Capacity: " + std::to_string(ts.getQueueCapacity()));
    THREADSYSTEM_DEBUG("  Is Busy: " + std::string(ts.isBusy() ? "Yes" : "No"));
    THREADSYSTEM_DEBUG("  Is Shutdown: " + std::string(ts.isShutdown() ? "Yes" : "No"));

    // Task statistics
    THREADSYSTEM_DEBUG("Task Statistics:");
    THREADSYSTEM_DEBUG("  Enqueued: " + std::to_string(ts.getTotalTasksEnqueued()));
    THREADSYSTEM_DEBUG("  Processed: " + std::to_string(ts.getTotalTasksProcessed()));

    size_t pending = ts.getTotalTasksEnqueued() - ts.getTotalTasksProcessed();
    THREADSYSTEM_DEBUG("  Pending: " + std::to_string(pending));
}
```

## Troubleshooting

### Common Issues

| Issue | Symptoms | Solution |
|-------|----------|----------|
| **High queue utilization** | Tasks queuing up, performance degradation | Increase worker count or optimize task granularity |
| **Worker thread starvation** | Some workers idle while others busy | Optimize task batching and WorkerBudget allocation |
| **Memory growth** | Increasing memory usage | Reserve queue capacity, optimize task captures |
| **Deadlocks** | System hanging | Avoid blocking operations in tasks |
| **Poor performance** | Low throughput | Check task granularity, use appropriate priorities |

### Performance Validation

```cpp
void validateThreadSystemPerformance() {
    // Test load balancing with large workload
    const size_t taskCount = 10000;
    std::atomic<size_t> completedTasks{0};

    auto startTime = std::chrono::steady_clock::now();

    for (size_t i = 0; i < taskCount; ++i) {
        ThreadSystem::Instance().enqueueTask([&]() {
            // Simulate work
            std::this_thread::sleep_for(std::chrono::microseconds(100));
            completedTasks.fetch_add(1, std::memory_order_relaxed);
        }, TaskPriority::Normal, "Load Test Task");
    }

    // Wait for completion
    while (completedTasks.load() < taskCount) {
        std::this_thread::sleep_for(std::chrono::milliseconds(10));
    }

    auto endTime = std::chrono::steady_clock::now();
    auto duration = std::chrono::duration_cast<std::chrono::milliseconds>(endTime - startTime);

    // Expected: <2000ms for 10,000 tasks on 8-core system
    THREADSYSTEM_INFO("Load test completed in " + std::to_string(duration.count()) + "ms");

    // Validate load balancing effectiveness
    if (duration.count() < 2000) {
        THREADSYSTEM_INFO("‚úÖ Excellent load balancing performance");
    } else {
        THREADSYSTEM_WARN("‚ö†Ô∏è Suboptimal performance - check WorkerBudget allocation");
    }
}
```

## Advanced Usage

### Custom WorkerBudget Integration

```cpp
class CustomSubsystem {
private:
    WorkerBudget calculateOptimalBudget() {
        size_t totalWorkers = ThreadSystem::Instance().getThreadCount();

        return WorkerBudget{
            .totalWorkers = totalWorkers,
            .engineReserved = std::max(1UL, totalWorkers / 10),      // 10%
            .aiAllocated = (totalWorkers * 6) / 10,                  // 60%
            .eventAllocated = (totalWorkers * 3) / 10,               // 30%
            .remaining = 0
        };
    }

public:
    void processWithBudget() {
        auto budget = calculateOptimalBudget();

        // Use allocated workers efficiently
        size_t workersToUse = budget.aiAllocated;
        if (getWorkloadSize() > 1000 && budget.hasBufferCapacity()) {
            workersToUse += std::min(budget.remaining, 2UL);
        }

        distributeTasks(workersToUse);
    }
};
```

### Task Pipeline Pattern

```cpp
class TaskPipeline {
public:
    template<typename T>
    auto stage1(const std::vector<T>& input) -> std::future<std::vector<T>> {
        return ThreadSystem::Instance().enqueueTaskWithResult([=]() {
            std::vector<T> result;
            for (const auto& item : input) {
                result.push_back(processStage1(item));
            }
            return result;
        }, TaskPriority::High, "Pipeline Stage 1");
    }

    template<typename T>
    auto stage2(std::future<std::vector<T>>&& input) -> std::future<std::vector<T>> {
        return ThreadSystem::Instance().enqueueTaskWithResult([input = std::move(input)]() mutable {
            auto data = input.get(); // Wait for previous stage
            std::vector<T> result;
            for (const auto& item : data) {
                result.push_back(processStage2(item));
            }
            return result;
        }, TaskPriority::Normal, "Pipeline Stage 2");
    }

    template<typename T>
    void execute(const std::vector<T>& input) {
        auto stage1Result = stage1(input);
        auto stage2Result = stage2(std::move(stage1Result));

        // Final processing
        ThreadSystem::Instance().enqueueTask([stage2Result = std::move(stage2Result)]() mutable {
            auto finalResult = stage2Result.get();
            processFinalResult(finalResult);
        }, TaskPriority::Normal, "Pipeline Final");
    }
};
```

## Production Readiness & Best Practices

### ‚úÖ Optimal Usage Patterns

**Large Batch Processing:**
```cpp
// ‚úÖ Process entities in large batches (1000+ items)
void AIManager::updateOptimal() {
    size_t workerCount = ThreadSystem::Instance().getThreadCount();
    size_t batchSize = entities.size() / workerCount;

    for (size_t i = 0; i < workerCount; ++i) {
        ThreadSystem::Instance().enqueueTask([=]() {
            processBatch(i * batchSize, batchSize);
        }, TaskPriority::Normal, "AI_Batch_" + std::to_string(i));
    }
}
```

**Priority-Based Task Submission:**
```cpp
// ‚úÖ Use appropriate priorities
ThreadSystem::Instance().enqueueTask(renderCriticalTask, TaskPriority::Critical);
ThreadSystem::Instance().enqueueTask(gameLogicTask, TaskPriority::Normal);
ThreadSystem::Instance().enqueueTask(backgroundTask, TaskPriority::Low);
```

**WorkerBudget-Aware Processing:**
```cpp
// ‚úÖ Adapt to available workers
WorkerBudget budget = calculateWorkerBudget(totalWorkers);
size_t optimalWorkers = budget.getOptimalWorkerCount(
    budget.aiAllocated, entityCount, 1000);
```

### ‚ùå Anti-Patterns to Avoid

**Excessive Small Tasks:**
```cpp
// ‚ùå Don't create tiny tasks
for (auto& entity : entities) {
    ThreadSystem::Instance().enqueueTask([&entity]() {
        entity.update(); // Too small!
    });
}
```

**Blocking Operations in Tasks:**
```cpp
// ‚ùå Don't block worker threads
ThreadSystem::Instance().enqueueTask([]() {
    std::this_thread::sleep_for(std::chrono::seconds(1)); // Bad!
    loadResourceFromDisk(); // Also bad!
});
```

**Ignoring Task Priorities:**
```cpp
// ‚ùå Don't use Normal priority for everything
ThreadSystem::Instance().enqueueTask(task, TaskPriority::Normal); // Consider priority!
```

### Performance Validation

**System Health Check:**
```cpp
void validateThreadSystemHealth() {
    auto& ts = ThreadSystem::Instance();

    // Check queue utilization
    double utilization = static_cast<double>(ts.getQueueSize()) / ts.getQueueCapacity();
    if (utilization > 0.8) {
        THREADSYSTEM_WARN("Queue utilization high: " + std::to_string(utilization * 100) + "%");
    }

    // Check worker efficiency
    size_t processed = ts.getTotalTasksProcessed();
    size_t enqueued = ts.getTotalTasksEnqueued();
    if (processed < enqueued * 0.95) {
        THREADSYSTEM_WARN("Worker efficiency below 95%");
    }
}
```

### Production Deployment Checklist

- [ ] **Memory Validation**: Confirm <1KB overhead on target hardware
- [ ] **Performance Testing**: Validate 90%+ efficiency under peak loads
- [ ] **Error Handling**: Test graceful degradation under high stress
- [ ] **Resource Monitoring**: Implement queue and worker utilization tracking
- [ ] **Platform Testing**: Verify behavior across all target platforms
- [ ] **Load Testing**: Test with 10,000+ tasks over extended periods

## Conclusion

The ThreadSystem provides a robust, high-performance foundation for multi-threaded game development. With intelligent WorkerBudget allocation, priority-based scheduling, and comprehensive performance monitoring, it enables scalable, maintainable concurrent programming.

### Key Takeaways

- **Simple API**: Fire-and-forget or result-returning task submission
- **Automatic Optimization**: WorkerBudget allocation and priority scheduling require no configuration
- **Production Ready**: Comprehensive error handling, thread safety, and monitoring
- **Engine Integrated**: Seamless integration with all engine subsystems
- **Performance Focused**: Optimized for game development workloads

The ThreadSystem transforms complex concurrent programming into simple task submission, enabling developers to focus on game logic while achieving optimal multi-core performance automatically.
